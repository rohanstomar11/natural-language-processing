{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4d6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "import pickle\n",
    "import numpy as np\n",
    "from package import process_tweets, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e024a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "all_tweets = positive_tweets + negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f43cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings = pickle.load(open('en_embeddings.p', 'rb')) #english word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26274916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_embedding(tweet, en_embeddings):\n",
    "    \n",
    "    doc_embedding = np.zeros(300)\n",
    "    \n",
    "    processed_doc = process_tweets(tweet)\n",
    "    \n",
    "    for word in processed_doc:\n",
    "        doc_embedding += en_embeddings.get(word, 0)\n",
    "    \n",
    "    return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d2d141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "\n",
    "tweet_embedding = get_document_embedding(custom_tweet, en_embeddings)\n",
    "tweet_embedding[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116d76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vecs(docs, embeddings):\n",
    "    \n",
    "    doc_dict = {}\n",
    "    \n",
    "    doc_vecs = []\n",
    "    \n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_embedding = get_document_embedding(doc, embeddings)\n",
    "        \n",
    "        doc_dict[i] = doc_embedding\n",
    "        \n",
    "        doc_vecs.append(doc_embedding)\n",
    "        \n",
    "    doc_matrix = np.vstack(doc_vecs)\n",
    "    \n",
    "    return doc_matrix, doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "527dc671",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_matrix, doc_dict = get_document_vecs(all_tweets, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315d1c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dictionary:  10000\n",
      "Document vecs shape:  (10000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dictionary: \", len(doc_dict))\n",
    "print(\"Document vecs shape: \", doc_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2ad3ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sad']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tweet = \"i am sad\"\n",
    "process_tweets(my_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bae67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_embedding = get_document_embedding(my_tweet, en_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca9252e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@zoeeylim sad sad sad kid :( it's ok I help you watch the match HAHAHAHAHA\n"
     ]
    }
   ],
   "source": [
    "max_id = np.argmax(cosine_similarity(doc_matrix, tweet_embedding))\n",
    "print(all_tweets[max_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "612fff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 300\n"
     ]
    }
   ],
   "source": [
    "N_VECS = len(all_tweets) #no. of vectors\n",
    "N_DIMS = len(doc_dict[1]) #dimension in each vector\n",
    "print(N_VECS,N_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68ce155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PLANES = 10 #no. of planes\n",
    "N_UNIVERSES = 25 #no. of times to repeat the hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ae82989",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "planes = [np.random.normal(size=(N_DIMS, N_PLANES)) for _ in range(N_UNIVERSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbdbec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
