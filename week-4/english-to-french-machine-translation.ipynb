{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc97770a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules import successfully\n"
     ]
    }
   ],
   "source": [
    "#Importing Modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "from package import get_dict, cosine_similarity\n",
    "\n",
    "print(\"Modules import successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fce830",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings = pickle.load(open('en_embeddings.p', 'rb')) #english word embeddings\n",
    "fr_embeddings = pickle.load(open('fr_embeddings.p', 'rb')) #french word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4ac2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of English Words:  6370\n",
      "No. of French Words:  5766\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of English Words: \", len(en_embeddings)) #checking the length of english embeddings\n",
    "print(\"No. of French Words: \", len(fr_embeddings)) #checking the length of french embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e93070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 English Words:  ['the', 'was', 'for', 'that', 'with', 'from', 'this', 'utc', 'his', 'not']\n",
      "10 French Words:  ['la', 'Ã©tait', 'pour', 'cela', 'avec', 'depuis', 'ce', 'tuc', 'son', 'pas']\n"
     ]
    }
   ],
   "source": [
    "print(\"10 English Words: \",list(en_embeddings.keys())[:10]) #first 10 english words\n",
    "print(\"10 French Words: \", list(fr_embeddings.keys())[:10]) #first 10 french words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b268d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_fr_train = get_dict(\"en-fr.train.txt\") #loading english to french training dictionary\n",
    "en_fr_test = get_dict(\"en-fr.test.txt\") #loading english to french test dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f61a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  5000\n",
      "Test:  1500\n"
     ]
    }
   ],
   "source": [
    "print(\"Training: \", len(en_fr_train)) #checking the length of training dictionary\n",
    "print(\"Test: \", len(en_fr_test)) #checking the length of test dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f782c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrices(en_fr, english_vecs, french_vecs):\n",
    "    \n",
    "    X_l=[] #initialising list to get embeddings for english words\n",
    "    Y_l=[] #initialising list to get embeddings for french words\n",
    "    \n",
    "    english_set = english_vecs.keys() #list of english words in embeddings\n",
    "    french_set = french_vecs.keys() #list of french words in embeddings\n",
    "    \n",
    "    for en_word, fr_word in en_fr.items(): \n",
    "        if en_word in english_set and fr_word in french_set: #checking if the english and french word have embeddings or not\n",
    "            \n",
    "            en_vec = english_vecs[en_word] #getting the embedding for english word\n",
    "            fr_vec = french_vecs[fr_word] #getting the embedding for french word\n",
    "            \n",
    "            X_l.append(en_vec) #add the embedding for english word\n",
    "            Y_l.append(fr_vec) #add the embedding for french word\n",
    "    \n",
    "    X = np.vstack(X_l) #making a matrix for all the  english word embeddings\n",
    "    Y = np.vstack(Y_l) #matrix for all the french word embeddings\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce4eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = get_matrices(en_fr_train, en_embeddings, fr_embeddings) #getting our training matrices for english and french words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90d7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(X, Y, R):\n",
    "    \n",
    "    m= X.shape[0] #total words in english embedding matrix\n",
    "    \n",
    "    diff = np.dot(X,R) - Y #calculating difference (XR-Y)\n",
    "    \n",
    "    diff_squared = diff**2 #absolute square for each element in our matrix (will be using for frobenius norm)\n",
    "    \n",
    "    sum_diff_squared = np.sum(diff_squared) #sum of all the squared elements\n",
    "    \n",
    "    loss = sum_diff_squared/m #calculating average loss (we took average loss so model wont be affected by variation in size of data)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf3cd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, R):\n",
    "    m = X.shape[0] #no. of rows in X\n",
    "    gradient = np.dot(X.transpose(), (np.dot(X,R)-Y))*2/m #calculating gradient (2/m*(X'.(XR-Y)))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcb6925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_embeddings(X, Y, steps=100, learning_rate=0.0003):\n",
    "    np.random.seed(11)\n",
    "    \n",
    "    R = np.random.rand(X.shape[1], X.shape[1]) #initialising R to random values\n",
    "    \n",
    "    for i in range(steps): \n",
    "        if i%25==0:\n",
    "            print(f\"loss at iteration {i} is: {compute_loss(X, Y, R):.4f}\") #printing loss after every 25th iteration\n",
    "            \n",
    "        grad = compute_gradient(X, Y, R) #calculate gradient\n",
    "        \n",
    "        R -= learning_rate*grad #updating R according to learning rate and gradient\n",
    "        \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df26df8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 3.4632\n",
      "loss at iteration 25 is: 3.3618\n",
      "loss at iteration 50 is: 3.2640\n",
      "loss at iteration 75 is: 3.1697\n",
      "loss at iteration 100 is: 3.0789\n",
      "loss at iteration 125 is: 2.9914\n",
      "loss at iteration 150 is: 2.9070\n",
      "loss at iteration 175 is: 2.8257\n"
     ]
    }
   ],
   "source": [
    "#validating our functions\n",
    "np.random.seed(11)\n",
    "m = 10\n",
    "n = 5\n",
    "X = np.random.rand(m, n)\n",
    "Y = np.random.rand(m, n)\n",
    "R = align_embeddings(X, Y, steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd39741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbor(v, candidates, k=1):\n",
    "    \n",
    "    similarity_l = [] #initalising similarity list\n",
    "    \n",
    "    for c in candidates:\n",
    "        cos_similarity = cosine_similarity(v, c) #cosine similarity for v and c\n",
    "        similarity_l.append(cos_similarity) #adding the current similarity to similarity list\n",
    "    \n",
    "    sorted_ids = np.argsort(similarity_l) #sorting the similarity list and getting their indices \n",
    "    \n",
    "    k_ids = sorted_ids[-k:] #selecting indices of k candidates with most similarity\n",
    "    \n",
    "    return k_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9db34058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9 9 9]\n",
      " [1 0 5]\n",
      " [2 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#validating our nearest neighbor function\n",
    "v = np.array([1, 0, 1])\n",
    "candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]]) \n",
    "print(candidates[nearest_neighbor(v, candidates, 3)]) #3 most similar candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c57d9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vocabulary(X, Y, R):\n",
    "    pred = np.dot(X,R) #predicting the embeddings for the english words with the help of transformation matrix\n",
    "    \n",
    "    num_correct = 0 #counter to count correct values\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_ids = nearest_neighbor(pred[i], Y) #getting the index of most similar word for our predicted embeddings\n",
    "        \n",
    "        if pred_ids == i: #if predicted index matches the i \n",
    "            num_correct += 1 #adding to correct counter\n",
    "    \n",
    "    accuracy = num_correct/ len(pred) #calculating accuracy for predictions\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aad3df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = get_matrices(en_fr_test, en_embeddings, fr_embeddings) #getting the matrices for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e4a9fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 969.5962\n",
      "loss at iteration 25 is: 97.7116\n",
      "loss at iteration 50 is: 26.8955\n",
      "loss at iteration 75 is: 9.8601\n",
      "loss at iteration 100 is: 4.4277\n",
      "loss at iteration 125 is: 2.3598\n",
      "loss at iteration 150 is: 1.4674\n",
      "loss at iteration 175 is: 1.0456\n",
      "loss at iteration 200 is: 0.8324\n",
      "loss at iteration 225 is: 0.7190\n",
      "loss at iteration 250 is: 0.6562\n",
      "loss at iteration 275 is: 0.6203\n",
      "loss at iteration 300 is: 0.5992\n",
      "loss at iteration 325 is: 0.5865\n",
      "loss at iteration 350 is: 0.5787\n",
      "loss at iteration 375 is: 0.5738\n"
     ]
    }
   ],
   "source": [
    "R_train = align_embeddings(X_train, Y_train, steps=400, learning_rate=0.8) #getting the transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d61fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.560\n"
     ]
    }
   ],
   "source": [
    "acc = test_vocabulary(X_val, Y_val, R_train) #calculating the accuracy on our data and transformation matrix\n",
    "print(f\"accuracy on test set is {acc:.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6d3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trial\n",
    "dict_trial = {\n",
    "    \"how\":\"comment\",\n",
    "    \"are\":\"son\",\n",
    "    \"you\":\"toi\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf8c67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial matrix\n",
    "X_trial, Y_trial = get_matrices(dict_trial, en_embeddings, fr_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bbf6280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 0 is: 306.3372\n",
      "loss at iteration 25 is: 71.9307\n",
      "loss at iteration 50 is: 17.1479\n",
      "loss at iteration 75 is: 4.1356\n",
      "loss at iteration 100 is: 1.0060\n",
      "loss at iteration 125 is: 0.2463\n",
      "loss at iteration 150 is: 0.0606\n",
      "loss at iteration 175 is: 0.0149\n",
      "loss at iteration 200 is: 0.0037\n",
      "loss at iteration 225 is: 0.0009\n",
      "loss at iteration 250 is: 0.0002\n",
      "loss at iteration 275 is: 0.0001\n"
     ]
    }
   ],
   "source": [
    "#trial transformation matrix\n",
    "R_trial = align_embeddings(X_trial, Y_trial, steps = 300, learning_rate=0.006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fbd3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy on trial\n",
    "acc_trial = test_vocabulary(X_trial, Y_trial, R_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffd49b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on trial data:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy on trial data: \",acc_trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
